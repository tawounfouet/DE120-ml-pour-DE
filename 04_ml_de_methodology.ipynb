{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<img src=\"https://assets-datascientest.s3-eu-west-1.amazonaws.com/train/logo_datascientest.png\" style=\"height:150px\">\n",
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<center><h1> Machine Learning pour les Data Engineers</h1></center>\n",
    "<center><h2>  Méthodologie : De la préparation à la modélisation </h2></center>\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    "<h3> 1. Introduction</h3>\n",
    "\n",
    "> Avant d'aboutir à l'étape d'entraînement du modèle, un long travail de préparation des données est requis, on parle de **Data Preparation** : nettoyage, encodage, mise à l'échelle et plus encore. Ce notebook a pour but de détailler les étapes de la méthodologie à appliquer lorsque l'on souhaite créer un modèle de Machine Learning. Nous passerons donc en revue les étapes ainsi que les méthodes que vous pourrez utiliser (descriptions et illustrations). \n",
    ">\n",
    "> <h4> 1.1 Présentation des étapes de préparation </h4>\n",
    ">\n",
    "> Nous allons commencer par citer les principales étapes de la préparation des données.\n",
    "> \n",
    "> * Des **données brutes aux variables**\n",
    ">\n",
    "> Le format des données initial peut ne pas être optimal pour l'analyse et la modélisation des données. Il conviendra alors dans ce cas de créer un tableau de données avec les dites variables explicatives du modèle ainsi que la variable cible, celle que l'on cherchera à prédire.\n",
    ">\n",
    "> <img src=\"https://assets-datascientest.s3.eu-west-1.amazonaws.com/notebooks/module_da_121/notebook_methodologie/set_split.png\" style=\"height:300px\">\n",
    ">\n",
    ">On pourra être confronté à plusieurs cas de figure comme celui-ci par exemple : les données que l'on souhaite utiliser se trouvent dans diverses sources, il faut alors les rassembler à l'aide de jointures. Ici dans notre illustration on obtient après avoir travaillé la structuration de nos données, quatre variables explicatives et une variable cible. La variable cible, celle que l'on cherche à prédire, est ici une variable binaire qualitative. On prédit de faire partie d'une catégorie ou d'une autre. A titre d'exemple, **0** peut correspondre à la **catégorie \"Sain\"** et **1** à la **catégorie \"Malade\"**.\n",
    ">\n",
    "> * **Nettoyage de la donnée**\n",
    "> \n",
    "> Une fois rassemblées, les données peuvent avoir besoin d'être nettoyées. Caractères spéciaux, données manquantes, hétérogénité de la casse : tous ces cas de figure peuvent être un frein à la modélisation de nos données.\n",
    ">\n",
    "> * **Encodage**\n",
    ">\n",
    "> Les données à disposition ne sont pas toujours des données numériques : type catégoriel, textuel ou encore date, tous ces types de données nécessitent un traitement particulier pour les convertir en valeurs numériques car les algorithmes de Machine Learning prennent en entrée uniquement des valeurs numériques. Ce format de données est donc indispensable pour leur bon fonctionnement.\n",
    "> \n",
    "> * **Transformation**\n",
    ">\n",
    "> L'application de certains algorithmes de Machine Learning nécessite la vérification d'un ensemble d'hypothèses. Pour respecter ces hypothèses, il est souvent nécessaire d'appliquer des transformations à nos variables. On pourra par exemple citer la **transformation logarithmique**, la **standardisation** ou encore la **normalisation**. Il est à noter que tous les algorithmes n'ont pas besoin des mêmes traitements. Les modèles linéaires ou plus globalement ceux qui font intervenir la notion de distance sont souvent très sensibles à ces transformations qui vont alors être très importantes pour leur bon fonctionnement. En particulier, la mise à l'échelle des variables sera indispensable avant d'entraîner un modèle linéaire. \n",
    ">\n",
    "> <h4> 1.2 La fuite des données </h4>\n",
    ">\n",
    "> Une notion très importante lorsque l'on souhaite créer des modèles est la fuite de données. Un modèle a pour objectif suite à son déploiement (l'étape finale), d'être utilisé sur de nouvelles données afin de faire des prédictions. On rappelle que l'on prédit la variable cible qui est cette fois-ci **inconnue**. \n",
    ">\n",
    "> Dans le cadre de son développement, le modèle s'entraîne sur des données que l'on aura sélectionné. Pour évaluer les performances de façon objective, il est très important de disposer d'une certaine quantité de données sur lesquelles le modèle va être testé. Ces données ne doivent pas être **connues** par le modèle.\n",
    ">\n",
    "> Pour que cette étape se passe au mieux il faut être très rigoureux dans l'étape de préparation des données. Dès le départ, il faut veiller à ce qu'une partie des données soit mise de côté. Si ce n'est pas bien le cas, des informations contenues dans cette partie des données pourraient fuiter et participer à l'entraînement du modèle. Cela biaiserait alors les résultats du modèle lors de son évaluation. **La toute première étape est donc de séparer notre jeu de données en une partie d'entraînement et de test.**\n",
    "\n",
    "<h3> 2. Les techniques de préparation des données </h3>\n",
    "\n",
    "> <h4> 2.1 La séparation du jeu de données </h4>\n",
    ">\n",
    "> On sépare le jeu de données en un **jeu d'entraînement** et un **jeu de test**. Comme expliqué dans le notebook introductif cette étape est indispensable au bon déroulement de tout le processus de modélisation. Sans ce jeu de test, impossible de savoir comment se comportera un modèle face à de nouvelles données. Selon la nature et la quantité des données à disposition, le volume de ces deux jeux de données peut varier. On retrouvera néanmoins très souvent la répartition **80/20** ou **70/30**. \n",
    ">\n",
    "><img src=\"https://assets-datascientest.s3.eu-west-1.amazonaws.com/notebooks/module_da_121/notebook_methodologie/tts_radient.png\" style=\"height:300px\">\n",
    ">\n",
    "> Pour la séparation de votre jeu de données, vous pourrez tout simplement utiliser la [fonction `train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) de la librairie **`scikit-learn`**. Voici la syntaxe : \n",
    "> \n",
    "> ```python \n",
    ">\n",
    "> from sklearn.model_selection import train_test_split\n",
    ">\n",
    "> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    ">\n",
    "> # X correspond au DataFrame des variables explicatives\n",
    ">\n",
    "> # y correspond à la variable cible\n",
    ">\n",
    "> ```\n",
    ">\n",
    "> <h4> 2.2 Le nettoyage des données </h4>\n",
    "> \n",
    "> Une fois que les jeux de données sont séparés, on peut commencer à étudier la qualité des données. La qualité globale du jeu de données est en règle générale traitée lors de la récupération et la structuration des données. Suite à cette étape préliminaire, les données ne doivent plus comporter de problèmes majeurs : noms de colonnes, décalage entre les variables, etc. \n",
    ">\n",
    "> Nous pouvons alors nous concentrer sur l'étude du contenu des variables : gestion des types de variables, des valeurs manquantes, valeurs aberrantes, parmi d'autres.\n",
    ">\n",
    "> <h5> 2.2.1 La gestion des valeurs manquantes </h5>\n",
    ">\n",
    "> La présence de valeurs manquantes est une problématique très récurrente. Il existe plusieurs techniques pour les traiter. Trois choix s'offrent à nous : les **conserver**, les **remplacer** ou les **supprimer**. \n",
    ">\n",
    "> * 1. Les conserver.\n",
    ">\n",
    "> > Dans certains cas, la conservation des valeurs manquantes peut être pertinente. En effet, l'absence de valeurs peut nous **apporter de l'information**. \n",
    "> >\n",
    "> > Un exemple simple permet d'illustrer ce cas de figure. \n",
    "> > Imaginons que nous disposons d'une base de données clients. On souhaite utiliser cette base de données pour créer un modèle qui nous indiquerait quels sont les clients susceptibles d'être intéressés par une nouvelle offre. La variable qui nous intéresse est ici celle du **numéro de téléphone**. \n",
    ">>\n",
    ">> <img src=\"https://assets-datascientest.s3.eu-west-1.amazonaws.com/notebooks/module_da_121/notebook_methodologie/missing.png\" style=\"height:300px\">\n",
    "> >\n",
    "> > Cette variable comporte des valeurs manquantes. L'absence de certaines valeurs au sein de cette variable peut se révéler très importante pour le modèle car elle pourrait nous apporter de l'information sur l'intérêt et la confiance que le client porte à l'entreprise. Cela peut indiquer tout simplement qu'il a refusé de communiquer cette information. \n",
    "> >\n",
    "> > Dans ce cas, il faut alors vérifier à l'aide de tests statistiques quel pourrait être l'apport de la conservation de cette information par rapport à la variable cible. \n",
    ">\n",
    ">\n",
    "> * 2. Les remplacer.\n",
    "> \n",
    "> > Dans le cas où elles sont peu nombreuses, on peut décider de remplacer les valeurs manquantes. Il y a plusieurs types d'indicateurs qui peuvent se révéler pertinent : \n",
    "> >\n",
    "> > > * La moyenne.\n",
    "> > >\n",
    "> > >\n",
    "> > > * La médiane.\n",
    "> > >\n",
    "> > >\n",
    "> > > * Le mode.\n",
    "> >\n",
    "> > Il existe néanmoins bien d'autres solutions pour remplacer ces valeurs. Une étude du reste du jeu de données peut nous permettre de les remplacer de façon plus avisée. Il est également possible de les remplacer de façon aléatoire selon la distribution des valeurs. \n",
    "> > L'élément le plus important à retenir est qu'il n'y a pas de mode d'emploi, l'important est de choisir la meilleure solution en étudiant attentivement nos variables. En remplaçant les valeurs manquantes nous pouvons introduire du biais, il faut donc être très vigilant lors de cette étape pour que ce retraitement n'impacte pas négativement notre jeu de données. \n",
    "> > Dans les précédents modules vous avez pu utiliser pour les remplacer, la [méthode `fillna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html) de la classe **`DataFrame`** de la librairie **`pandas`**. Voici la syntaxe : \n",
    "> >\n",
    "> > ```python \n",
    "> >\n",
    "> > # Soit X un DataFrame, des variables explicatives, on impute par la moyenne de chacune des variables\n",
    "> >\n",
    "> > X.fillna(X.mean(), inplace=True)\n",
    "> >\n",
    "> > ```\n",
    "> >\n",
    "> * 3. Les supprimer.\n",
    "> \n",
    "> > Plusieurs cas de figures peuvent entraîner la suppression de données manquantes. \n",
    "> > >\n",
    "> > > * L'absence de données peut indiquer un problème de qualité de données.\n",
    "> > > \n",
    "> > >\n",
    "> > > * Une trop grande quantité de données manquantes peut rendre l'utilisation d'une variable impossible.\n",
    "> > \n",
    "> > La suppression des valeurs manquantes peut être perçue comme la solution la plus simple à adopter. En revanche, elle n'est pas toujours pertinente, d'autant plus que les données sont une **denrée rare**. Supprimer des lignes où l'on peut trouver des données manquantes pourrait faire perdre énormément d'informations. Une grande vigilance est donc à apporter lors de cette étape. \n",
    "> > Pour les supprimer, vous connaissez déjà la [méthode `dropna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) de la classe **`DataFrame`** de la librairie **`pandas`**. Voici la syntaxe que vous pourrez employer : \n",
    "> >\n",
    "> > ```python \n",
    "> >\n",
    "> > # Soit X un DataFrame, des variables explicatives, on impute par la moyenne de chacune des variables\n",
    "> >\n",
    "> > X.dropna(inplace=True)\n",
    "> >\n",
    "> > ```\n",
    ">\n",
    "> <h5>  2.2.2 La gestion des valeurs aberrantes ou extrêmes dites aussi outliers</h5> \n",
    ">\n",
    "> Le premier élément à considérer est la différence de signification lorsque l'on parle de valeurs aberrantes ou de valeurs extrêmes. \n",
    ">\n",
    "> Prenons l'exemple d'une variable d'âge, si au sein de cette variable on se retrouve avec le chiffre 500, alors c'est **une valeur aberrante** car cette valeur ne s'explique pas. Les gens vivent aujourd'hui rarement au dessus de 100 ans. \n",
    "> <img src=\"https://assets-datascientest.s3.eu-west-1.amazonaws.com/notebooks/module_da_121/notebook_methodologie/non_sense_values.png\" style=\"height:300px\">\n",
    ">\n",
    "> Considérons maintenant une variable qui contient les salaires mensuels d'un groupe d'individus. La majorité des individus du groupe gagnent **entre 1500€ et 3500€**. S'il existe dans cette variable des salaires plus bas tels que 600€ ou plus haut autour de 10000€, on ne peut pas dire que ce sont des valeurs aberrantes. Ce sont des valeurs extrêmes. En effet, ces valeurs font sens mais n'atteignent pas ou dépassent l'intervalle des valeurs les plus représentées.\n",
    ">\n",
    "> <img src=\"https://assets-datascientest.s3.eu-west-1.amazonaws.com/notebooks/module_da_121/notebook_methodologie/extreme_values.png\" style=\"height:300px\">\n",
    ">\n",
    "> Dans le premier cas, il est important de corriger cette valeur ou de supprimer l'information car elle est erronée. Dans le second cas, la choix n'est pas simple car l'information a de fortes probabilités d'être correcte. En revanche si on la laisse, son caractère **rare** peut réduire les performances du modèle ou encore l'empêcher de converger.\n",
    ">\n",
    "> Considérons deux types de variables : **continue** et **discrète**. \n",
    ">\n",
    "> * <u>Variable continue : </u> Une variable continue prend un nombre infini ou non dénombrable de valeurs.\n",
    ">\n",
    ">\n",
    "> * <u>Variable discrète : </u> Une variable discrète prend un nombre fini ou dénombrable de valeurs.\n",
    ">\n",
    "> Dans le cas d'une variable continue, c'est l'étude de la distribution des valeurs qui va nous permettre de détecter les outliers. \n",
    ">\n",
    "> Une façon simple de traiter cet aspect est de calculer les **quantiles ou les percentiles** sur notre variable.\n",
    "> \n",
    "> Une bonne solution dans ces cas est de corriger ces valeurs, avec une valeur plus juste ou bien si la qualité de la donnée est en doute de supprimer la ligne.\n",
    ">\n",
    "> Dans le cas des valeurs discrètes, que nous considèrerons comme des variables catégorielles ici, il faudra regarder attentivement si une ou plusieurs catégories ne sont pas sous ou sur-représentées. \n",
    "> Une solution est de travailler sur la répartition de ces catégories. Si une catégorie n'apparaît qu'un nombre infime de fois par rapport aux autres, on peut par exemple prendre le décision de la supprimer. Aussi, si une catégorie est sur-représentée au sein d'une variable, on peut décider de ne pas conserver cette modalité. Un extrême vigilance est néanmoins requise lorsque l'on traite ce genre de variables. La nature et la signification des variables peut justifier leur conservation. Un modèle de détection de fraude est un exemple parfait du cas où la surreprésentation d'une variable doit être traité avec attention. \n",
    ">\n",
    "> De plus, on notera que ces corrections ont lieu dans le but de la création de modèles de Machine Learning qui nécessitent pour la plupart de longues étapes de préparation des données. Ces corrections pourront être différentes selon comment nous souhaitons travailler avec les données (analyses, ...).\n",
    ">\n",
    "> <h4> 2.3 Encodage des données </h4>\n",
    ">\n",
    "> Comme nous le précisons dans la partie introductive de ce notebook, une variable de type catégoriel et textuel devra être réencodée. En effet, la plupart des algorithmes de Machine Learning ne prennent en entrée que des variables numériques. Il existe plusieurs techniques d'encodage, nous en citons deux ici : l'Ordinal Encoding et le One Hot Encoding. \n",
    ">\n",
    "> <h5> 2.3.1 Le One Hot Encoding </h5>\n",
    ">\n",
    "> Une variable indicatrice est une variable qualitative binaire prenant les valeurs 0 ou 1. Ces variables sont construites à partir des catégories qui composent des variables catégorielles (de plus de 2 catégories). Cette technique d'encodage s'appelle le <b> One Hot Encoding </b>. \n",
    ">\n",
    "> Lorsqu'il y a exactement 2 catégories, il n'est pas utile de créer plusieurs nouvelles variables, il suffit de remplacer une valeur par 0 et l'autre par 1. En laissant ces deux variables, l'information serait redondante. \n",
    ">\n",
    "> Prenons l'exemple d'une variable avec 3 catégories, lui appliquer cette technique nous permettra d'obtenir 3 variables binaires. Pour la première (catégorie 1), la variable vaudra 1 si un individu correspond à la catégorie 1, 0 sinon. Le procédé est le même pour les deux autres catégories.    \n",
    ">\n",
    "> <img src=\"https://assets-datascientest.s3.eu-west-1.amazonaws.com/notebooks/module_da_121/notebook_methodologie/ohe_fr.png\" style=\"height:300px\">\n",
    ">\n",
    "> Bien que ce procédé nous retourne ici dans notre exemple 3 nouvelles variables il est important de noter qu'au sein d'un modèle on ne peut pas toutes les conserver. En effet, il faudra conserver <b>N-1 variables</b> sans quoi nous serions confronté à un problème de <b>multicolinéarité</b> qui nuira aux performances de notre modèle. Ce problème de multicolinéarité provient du fait que l'on peut prédire l'occurence de la troisième variable (troisième catégorie) à l'aide des deux autres catégories.\n",
    ">\n",
    ">\n",
    "> <h5> 2.3.2 L'Ordinal Encoding </h5>\n",
    ">\n",
    "> Au lieu de créer plusieurs variables issue d'une variable catégorielle, l'idée est ici de transformer les catégories de notre variable par des valeurs. Cette transformation est pertinente lorsqu'il existe un ordre entre les catégories mais elle peut toujours être substituée par la technique de One Hot Encoding.\n",
    ">\n",
    "> Prenons par exemple une variable de **noms de fruits**, il y a trois catégories : Prune, Poire et Pomme. Si ces catégories sont remplacées par des chiffres (0, 1, 2), cela n'aura pas de sens car il n'existe pas un ordre entre ces fruits. Dans ce cas de figure, on ne pourra utiliser que la technique de One Hot Encoding.\n",
    ">\n",
    "> En revanche, si l'on a cette fois-ci une variable qui évalue la **satisfaction d'un utilisateur**. Les catégories qu'elle contient sont : Peu Satisfait, Satisfait et Très Satisfait. On pourra cette fois-ci réencodée notre variable de façon ordinale car il existe un ordre entre ces catégories. On aura :\n",
    ">\n",
    "> * Peu Satisfait = 0\n",
    "> * Satisfait = 1\n",
    "> * Très Satisfait = 2\n",
    ">\n",
    "> <img src=\"https://assets-datascientest.s3.eu-west-1.amazonaws.com/notebooks/module_da_121/notebook_methodologie/ordinal_encoding.png\" style=\"height:300px\">\n",
    ">\n",
    "> <div class=\"alert alert-warning\">\n",
    "    <i class=\"fa fa-info-circle\"></i> &emsp; Pour respecter l'ordre logique entre les catégories, il faudra faire attention à bien le spécifier. En l'absence de valeurs numériques, cet ordre est inconnu au préalable.</div>\n",
    ">\n",
    "> <h4> 2.4 Transformation des données </h4>\n",
    ">\n",
    "> Il existe de nombreuses techniques de mise à l'échelle dite aussi *scaling*, les plus connus sont la **normalisation** et la **standardisation**.\n",
    ">\n",
    "> ##### Normalisation entre 0 et 1\n",
    ">\n",
    "> La normalisation consiste à **borner** toutes les valeurs d'une variable **entre 0 et 1**. La démarche est simple pour chaque variable on retranche à chaque valeur la valeur minimale de la variable et on divise par la différence entre la valeur maximale et la valeur minimale. \n",
    ">\n",
    "><br>\n",
    "> <center>$ \\Large\\frac{x_i - min(x)}{max(x) - min(x)} $</center>\n",
    ">\n",
    "> ```python\n",
    ">\n",
    "> # Soit X un DataFrame, des variables explicatives\n",
    ">\n",
    "> (X - X.min()) / (X.max() - X.min())\n",
    "> ```\n",
    ">\n",
    "> Cette technique est à privilégier lorsque les variables ne suivent pas une distribution normale.\n",
    ">\n",
    "> ##### Standardisation (Normalisation Z-Score)\n",
    ">\n",
    "> Au contraire de la normalisation, il est recommandé d'appliquer la standardisation lorsque la distribution de la variable suit une loi normale. La standardisation permet de retrancher à chaque valeur la valeur moyenne de la variable et de diviser par son écart type. \n",
    ">\n",
    "> <br>\n",
    "> <center>$ \\Large\\frac{x_i - mean(x)}{std(x)} $</center>\n",
    ">\n",
    "> ```python\n",
    ">\n",
    "> # Soit X un DataFrame, des variables explicatives\n",
    ">\n",
    "> (X - X.mean()) / (X.std())\n",
    "> ```\n",
    ">\n",
    "> ##### Outliers\n",
    ">\n",
    "> En cas de présence d'outliers, l'application des deux techniques précédentes n'est pas appropriée puisqu'elles sont sensibles aux valeurs extrêmes (minimum, moyenne et maximum). Dans ce cas, la technique de Robust Scaling s'avère plus pertinente puisqu'elle s'appuie sur le premier, second (médiane) et troisième quartile de la distribution, qui ne sont pas sensibles aux valeurs extrêmes. <br>\n",
    "> La formule est celle-ci : \n",
    ">\n",
    "> <br>\n",
    "> <center>$ \\Large\\frac{x_i - median(x)}{q3(x) - q1(x)} $</center> <br>\n",
    ">\n",
    "> ```python\n",
    ">\n",
    "> # Soit X un DataFrame, des variables explicatives\n",
    "> \n",
    "> (X - X.median()) / (X.quantile(q=0.75) - X.quantile(q=0.25))\n",
    "> ```\n",
    "\n",
    "<h3> 3. L'intégration des étapes de préparation dans le cadre de la modélisation </h3>\n",
    "\n",
    "> <h4> 3.1 Les méthodes fit_transform et transform </h4>\n",
    "> \n",
    "> Nous présentons dans cette partie les méthodes **`fit_transform`** et **`transform`** qui vont vous simplifier l'intégration des étapes de préparation dans le cadre de la création d'un modèle de Machine Learning. En effet, il existe de nombreux outils **`scikit-learn`** qui vont nous permettre de **réaliser des modifications sur le jeu d'entraînement et de test**, de façon rigoureuse, en quelques lignes de code. Il existe un réel avantage à utiliser ces méthodes plutôt que les méthodes et fonctions **`pandas`** ou **`numpy`**, elles sont **spécialement adaptées** au Machine Learning. \n",
    ">\n",
    ">\n",
    "> La première chose dont il faut se **souvenir** lorsque l'on utilise les méthodes **`fit_transform`** et **`transform`** est la notion de **fuite de données**. \n",
    ">\n",
    "> Les transformations sont calibrées sur le **jeu d'entraînement** (**`fit_transform`**) et ensuite appliquées sur le **jeu de test** (**`transform`**). Lorsque l'on réalise l'entraînement, puisque l'on a à disposition le jeu de test (variables explicatives et variable cible), on peut être tenté de calibrer les transformations sur **le jeu de données entier**. Si l'on procède de cette façon, cela pourra entraîner une **fuite de données** et donc une **augmentation artificielle de la performance du modèle**. \n",
    ">\n",
    "> Il existe également un autre cas de figure important à noter. Lorsque l'on applique un **`OneHotEncoding`**, on crée de nouvelles variables à partir des catégories d'une ou plusieurs variables. Dans ce cas, il est également primordial d'appliquer un **`transform`** et non pas un **`fit_transform`** sur le jeu de test ou toutes nouvelles données sur lesquelles on applique le modèle. En effet, l'ensemble des variables explicatives a été fixé lors de l'entraînement du modèle. Imaginons qu'une nouvelle catégorie soit ajoutée dans le jeu de test ou les nouvelles données, alors **le modèle ne pourra pas s'appliquer** car l'ensemble des variables explicatives sera alors différent. \n",
    "> \n",
    "> La syntaxe sera toujours la suivante. Nous prenons ici en exemple le StandardScaler qui nous permet d'appliquer la transformation Z-Score à nos données. \n",
    ">\n",
    "> ```python\n",
    "> \n",
    "> from sklearn.preprocessing import StandardScaler\n",
    "> \n",
    "> # Soient X_train et X_test, les jeux, respectivement d'entraînement et de test, des variables explicatives\n",
    ">\n",
    "> scaler = StandardScaler()\n",
    ">\n",
    "> X_train_scaled = scaler.fit_transform(X_train)\n",
    ">\n",
    "> X_test_scaled = scaler.transform(X_test)\n",
    ">\n",
    "> ```\n",
    ">\n",
    "> Concrètement dans ce cas de figure, la moyenne et l'écart type des valeurs de chacune des variables est calculée sur le jeu d'entraînement. La transformation est faite sur le jeu d'entraînement. Puis dans un second temps, à partir des indicateurs calculés sur le jeu d'entraînement, on effectuera les transformations sur le jeu de test. \n",
    ">\n",
    ">\n",
    "> Les trois parties suivantes seront une sorte de catalogue non exhaustifs de méthodes que vous pouvez utiliser pour appliquer les corrections présentées plus haut à l'aide des méthodes **`fit_transform`** et **`transform`**.\n",
    "> \n",
    ">\n",
    "> <h4> 3.2 Nettoyage des données </h4>\n",
    ">\n",
    "> <h5> 3.2.1 Gestion des données manquantes </h5>\n",
    ">\n",
    "> <br>\n",
    ">\n",
    "> * **La méthode SimpleImputer**\n",
    ">\n",
    "> Cette [méthode](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) vous permet d'imputer les valeurs manquantes au sein des données. Dans l'exemple ci-dessous, la stratégie d'imputation choisie est la moyenne : **`strategy=\"mean\"`**. D'autres stratégies pourraient évidemment choisies : médiane (**`strategy=\"median\"`**), mode (**`strategy=\"most_frequent\"`**), etc.\n",
    ">\n",
    "> ```python\n",
    "> \n",
    "> from sklearn.impute import SimpleImputer\n",
    "> \n",
    "> # Soient X_train et X_test, les jeux, respectivement d'entraînement et de test, des variables explicatives\n",
    "> \n",
    "> import numpy as np \n",
    ">\n",
    "> imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    ">\n",
    "> X_train_imputed = imputer.fit_transform(X_train)\n",
    ">\n",
    "> X_test_imputed = imputer.transform(X_test)\n",
    ">\n",
    "> ```\n",
    "> Nous ne les présenterons pas ici mais il existe d'autres méthodes d'imputation telles que le [KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html?highlight=imputer#sklearn.impute.KNNImputer) ou encore le [IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html?highlight=imputer#sklearn.impute.IterativeImputer).\n",
    "> \n",
    "> <h5> 3.2.2 Gestion des outliers </h5>\n",
    ">\n",
    ">\n",
    "> On rappelle qu'il y a deux cas de figure : \n",
    ">\n",
    ">\n",
    "> * La distribution comprend des valeurs extrêmes qui sont **cohérentes**. \n",
    ">\n",
    ">\n",
    "> * La distribution comprend des valeurs aberrantes qui sont des **erreurs**.\n",
    ">\n",
    ">\n",
    "> Comme précisé plus haut, il est compliqué de trouver une solution dans le cas où les valeurs sont cohérentes. En effet, les remplacer peut causer une perte d'information. Il faut donc les traiter au cas par cas. Pour le cas où ces outliers sont des erreurs, il convient de les remplacer. Pour cela nous pouvons également utiliser un transformer. \n",
    ">\n",
    ">\n",
    "> Afin de délimiter les bornes \"acceptables\" de la distribution des valeurs, on peut calculer l'écart interquartile (IQR). L'écart interquartile correspond à la différence entre la valeur du 3e et du 1er quartile :  \n",
    ">\n",
    "> $ $\n",
    "> <center>$\\mbox{IQR} = \\mbox{Q3} -\\mbox{Q1}$</center>  \n",
    ">\n",
    "> $ $\n",
    "> L'intervalle que l'on va considérer comme \"valable\" correspond en règle générale à :  \n",
    "> $ $\n",
    ">\n",
    "> <center>$[\\mbox{Q1} - (1.5 * \\mbox{IQR}) \\mbox{;}  \\mbox{Q3} + (1.5 * \\mbox{IQR})]$</center>\n",
    "> $ $\n",
    ">\n",
    "> Plutôt que d'utiliser un transformer déjà existant nous allons en créer un pour traiter ces outliers en considérant l'écart interquartile. \n",
    "> > ```python\n",
    "> > \n",
    "> > from sklearn.base import BaseEstimator, TransformerMixin\n",
    "> >\n",
    "> > class IQR(BaseEstimator, TransformerMixin): \n",
    "> > \n",
    "> >   def __init__(self):\n",
    "> >       return None\n",
    "> >    \n",
    "> >     def fit(self, X, y = None):\n",
    "> >         self.q1 = X.quantile(0.25)    \n",
    "> >       self.q3 = X.quantile(0.75) \n",
    "> >       self.iqr = self.q3 - self.q1\n",
    "> >       return self\n",
    "> >  \n",
    "> >   def transform(self, X, y = None):\n",
    "> >       def test_bound(element):\n",
    "> >           if element<(self.q1-self.iqr*1.5):\n",
    "> >               element = self.q1 - 1.5*self.iqr\n",
    "> >           elif element>(self.q3+self.iqr*1.5):\n",
    "> >               element=self.q3 + 1.5*self.iqr\n",
    "> >           return element\n",
    "> >       return X.apply(lambda x : test_bound(x))\n",
    "> > ```\n",
    ">\n",
    "> Pour cela, nous créons une classe qui hérite de **`BaseEstimator`** et **`TransformerMixin`**. Ici, l'idée n'est pas de développer la création de ce transformer, simplement de montrer qu'il est possible de créer le transformer que l'on souhaite en quelques lignes.\n",
    ">\n",
    "> De la même façon que les transformers déjà implémentés, la syntaxe est celle-ci : \n",
    "> > ```python\n",
    "> > # Soient X_train et X_test, les jeux, respectivement d'entraînement et de test, des variables explicatives\n",
    "> >\n",
    "> > iqr = IQR()\n",
    "> >\n",
    "> > X_train_iqr = iqr.fit_transform(X_train)\n",
    "> >\n",
    "> > X_test_iqr = iqr.transform(X_test)\n",
    "> >\n",
    "> > ```\n",
    ">\n",
    "> <h4> 3.3 Encodage </h4>\n",
    "> \n",
    "> Dans la partie précédente, nous avons présenté deux techniques d'encodage : le One Hot Encoding et l'Ordinal Encoding. \n",
    "> Il existe deux transformers qui nous permettent d'effectuer ces transformations très simplement : **`OneHotEncoder`** et **`OrdinalEncoder`**. Nous ne l'avons pas évoqué plus tôt mais la variable cible peut également avoir besoin d'être ré-encodée. Pour cela il faut utiliser un transformer spécifique, c'est le **`LabelEncoder`**. Le fonctionnement est néanmoins le même que pour l'**`OrdinalEncoder`**. \n",
    ">\n",
    "> Voici les méthodes dédiées sur **`scikit-learn`** : \n",
    ">\n",
    "> * Le One Hot Encoding\n",
    "> \n",
    "> >  ```python \n",
    "> > from sklearn.preprocessing import OneHotEncoder\n",
    "> >\n",
    "> > # Le paramètre drop permet d'éviter le problème de multicolinéarité\n",
    "> > ohe = OneHotEncoder( drop=\"first\", sparse=False)\n",
    "> >\n",
    "> > ohe.fit_transform(X_train)\n",
    "> >\n",
    "> > ohe.transform(X_test)\n",
    "> > ```\n",
    ">\n",
    "> * L'Ordinal Encoding\n",
    ">\n",
    "> >  ```python \n",
    "> > from sklearn.preprocessing import OrdinalEncoder\n",
    "> >\n",
    "> > import numpy as np\n",
    "> >\n",
    "> > oe = OrdinalEncoder(unknown_value = np.nan, categories = [\"Peu Satisfait\", \"Satisfait\", \"Très Satisfait\"])\n",
    "> >\n",
    "> > oe.fit_transform(X_train)\n",
    "> >\n",
    "> > oe.transform(X_test)\n",
    "> > ```\n",
    ">\n",
    "> * Le Label Encoding\n",
    ">\n",
    "> >  ```python \n",
    "> > from sklearn.preprocessing import LabelEncoder\n",
    "> >\n",
    "> > le = LabelEncoder()\n",
    "> >\n",
    "> > le.fit_transform(y_train)\n",
    "> >\n",
    "> > le.transform(y_test)\n",
    "> > ```\n",
    ">\n",
    "> <h4> 3.4 Transformation </h4>\n",
    ">\n",
    "> Dans la partie précédente nous avons présenté trois transformations pour mettre à l'échelle nos données : la standardisation, la normalisation et le RobustScaling.\n",
    ">\n",
    "> Voici les méthodes dédiées sur **`scikit-learn`** : \n",
    "> \n",
    "> * La standardisation \n",
    "> > ```python\n",
    "> > from sklearn.preprocessing import StandardScaler\n",
    "> >\n",
    "> > # Soient X_train et X_test, les jeux, respectivement d'entraînement et de test, des variables explicatives\n",
    "> >\n",
    "> > scaler = StandardScaler()\n",
    "> >\n",
    "> > X_train_scaled = scaler.fit_transform(X_train)\n",
    "> >\n",
    "> > X_test_scaled = scaler.transform(X_test)\n",
    "> > ```\n",
    ">\n",
    "> * La normalisation \n",
    "> > ```python\n",
    "> > from sklearn.preprocessing import MinMaxScaler\n",
    "> >\n",
    "> > # Soient X_train et X_test, les jeux, respectivement d'entraînement et de test, des variables explicatives\n",
    "> >\n",
    "> > scaler = MinMaxScaler()\n",
    "> >\n",
    "> > X_train_scaled = scaler.fit_transform(X_train)\n",
    "> >\n",
    "> > X_test_scaled = scaler.transform(X_test)\n",
    "> > ```\n",
    ">\n",
    "> * Le RobustScaling \n",
    "> > ```python\n",
    "> > from sklearn.preprocessing import RobustScaler\n",
    "> >\n",
    "> > # Soient X_train et X_test, les jeux, respectivement d'entraînement et de test, des variables explicatives\n",
    "> >\n",
    "> > scaler = RobustScaler()\n",
    "> >\n",
    "> > X_train_scaled = scaler.fit_transform(X_train)\n",
    "> >\n",
    "> > X_test_scaled = scaler.transform(X_test)\n",
    "> > ```\n",
    "\n",
    "\n",
    "<h3> 4. L'entraînement, la prédiction et la mesure de performance du modèle </h3>\n",
    "\n",
    "> <h4>4.1 La modélisation avec l'API scikit-learn  </h4>\n",
    ">\n",
    "> Dans le second notebook de ce module, l'API **`scikit-Learn`** vous a été présentée. En quelques lignes de code, vous pouvez entraîner et évaluer un modèle de Machine Learning. \n",
    ">\n",
    "> Pour rappel, ce sont les méthodes : **`fit`**, **`predict`** et **`score`** qui seront ici utilisées pour respectivement : entraîner, prédire et mesurer la performance du modèle entraîné. \n",
    ">\n",
    "> Suite à la séparation du jeu de données en un jeu d'entraînement et de test puis l'application des étapes de préparation, nous avons à disposition ces quatre objets : **`X_train`**, **`X_test`**, **`y_train`** et **`y_test`**. \n",
    ">\n",
    "> > * **`X_train`** : Ce sont les variables explicatives qui vont servir à l'entraînement du modèle.\n",
    "> >\n",
    "> >\n",
    "> > * **`y_train`** : C'est la variable cible qui va servir à l'entraînement du modèle.\n",
    "> >\n",
    "> >\n",
    "> > * **`X_test`** : Ce sont les variables explicatives qui vont servir à la mesure de performance du modèle.\n",
    "> > \n",
    "> >\n",
    "> > * **`y_test`** : C'est la variable cible qui va servir à la mesure de performance du modèle.\n",
    ">\n",
    "> Imaginons que nous ayons affaire à une problématique de classification supervisée et que nous voulions utiliser l'algorithme de régression logistique. La syntaxe sera celle-ci : \n",
    ">\n",
    ">\n",
    "> > ```python\n",
    "> > from sklearn.linear_model import LogisticRegression\n",
    "> >\n",
    "> > model = LogisticRegression() # On instancie le modèle\n",
    "> >\n",
    "> > model.fit(X_train, y_train) # On entraîne le modèle\n",
    "> >\n",
    "> > y_pred = model.predict(X_test) # On obtient la prédiction du modèle\n",
    "> >\n",
    "> > model.score(X_test, y_test) # On calcule le score du modèle en comparant la prédiction et la valeur vraie\n",
    "> >```\n",
    ">\n",
    "> Pour rappel lorsque l'on évalue la performance du modèle on compare la prédiction de notre modèle entraîné sur le jeu de test (**`X_test`**) et la vraie valeur de la variable cible du jeu de test (**`y_test`**). Derrière la méthode **`score`**, il y a donc la prédiction de cette variable qui est réalisée, de la même façon que lorsque l'on utilise la méthode **`predict`**.\n",
    "\n",
    "<h3> 5. La pipeline scikit-Learn </h3>\n",
    "\n",
    "> Nous venons de voir un grand nombre d'outils de la librairie **`scikit-Learn`** qui nous permettent de réaliser les transformations nécessaires à notre jeu de données de façon très synthétique et rigoureuse. \n",
    "> Ces étapes prises séparément peuvennt être longues à implémenter. \n",
    ">\n",
    "> Pour pallier ce problème, il existe une fonction du sous module **`sklearn.pipeline`** qui permet de rassembler toutes ces transformations au sein d'un même objet. C'est une pipeline. Voici une syntaxe possible : \n",
    ">\n",
    "> ```python\n",
    "> from sklearn.pipeline import Pipeline\n",
    ">\n",
    "> pipe = Pipeline(\n",
    "    [('simple_imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model_logistic', LogisticRegression())])\n",
    ">    \n",
    ">   pipe.fit(X_train, y_train)\n",
    ">\n",
    ">   predictions = pipe.predict(X_test)\n",
    ">\n",
    ">   print(pipe.score(X_test, y_test))\n",
    "> ```\n",
    ">\n",
    "> Comme vous pouvez l'observer dans le code, la pipeline s'entraîne sur le jeu d'entraînement et prédit sur le jeu de test. La syntaxe est ainsi la même qu'un modèle que l'on souhaite entraîné. Derrière son fonctionnement on retrouve les méthodes **`fit_transform`** et **`transform`**.\n",
    "\n",
    "<h3> 6. Conclusion </h3>\n",
    "\n",
    "> <h4>6.1 A retenir </h4>\n",
    ">\n",
    "> \n",
    "> * Les étapes de préparation des données sont nombreuses : nettoyage, encodage, transformation, etc. \n",
    ">\n",
    ">\n",
    "> * La séparation du jeu de données en un jeu d'entraînement et de test arrive donc en amont de toutes ces étapes de traitement des données.\n",
    ">\n",
    ">\n",
    "> * Les étapes de préparation des données doivent être appliquées de façon rigoureuse sue le jeu d'entraînement puis de test pour qu'il n'y ait pas de fuite de données ou d'écart avec les variables ayant servies à l'entraînement du modèle.\n",
    ">\n",
    ">\n",
    "> * Les méthodes **`fit_transform`** et **`transform`** permettent d'appliquer les transformations sur le jeu d'entraînement et le jeu de test de façon rigoureuse.\n",
    ">\n",
    ">\n",
    "> * L'utilisation d'une **`Pipeline`** permet de rassembler les étapes de préparation des données et la modélisation.\n",
    ">\n",
    ">\n",
    "> <h4>6.2 Et après ? </h4>\n",
    "> \n",
    "> Dans le prochain notebook nous allons voir un exemple d'application dans le cas d'une problématique de classification supervisée. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "fr"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "fr",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
